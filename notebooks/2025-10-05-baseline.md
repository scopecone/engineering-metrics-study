# Engineering Metrics Baseline · 2025-10-05 Snapshot

This draft notebook captures the first pass over the expanded OSS cohort so we can line it up against the Engineering Metrics Simulator assumptions.

## Cohort Overview
- **Tracked repos:** 280 (after pruning eight zero-signal repos in the latest discovery batch; see “Holdout adjustments”).
- **Metrics rows:** 280 (filtered to the current sample; one row per repo).
- **Observation window:** 365-day pull (Actions where available, otherwise Deployments/Releases).
- **Data freshness:** Incremental collector last run `2025-10-05T08:22Z`.

## Holdout adjustments
| Removed slug | Reason |
| --- | --- |
| adamcohenhillel/ADeus | No merged PRs in 365-day window |
| CMU-Perceptual-Computing-Lab/openpose | Releases stale since 2020 |
| FiloSottile/mkcert | Releases stale since 2022 |
| garylab/MakeMoneyWithAI | No merged PRs in window |
| Genymobile/scrcpy | PRs merge into `dev`, not default branch |
| google-research/bert | Archived upstream |
| ksm26/Multi-AI-Agent-Systems-with-crewAI | No merged PRs in window |
| LinShunKang/MyPerf4J | No merged PRs in window |

These repos now live in `config/repos.holdout.json` and their cached payloads have been purged.

## Deployment Frequency by Ecosystem
| Language | Repos | Median DF | P75 DF | P85 DF | P95 DF | Median PR (h) | P75 PR (h) |
| --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Go | 68 | **1.0** | 1.13 | 2.00 | 5.30 | 23.4 | 72.7 |
| TypeScript | 57 | **1.0** | 2.00 | 4.00 | 16.0 | **6.0** | 19.1 |
| Python | 46 | **1.0** | 2.75 | 5.00 | 28.4 | 14.2 | 22.6 |
| Java | 23 | **1.0** | 2.00 | 2.00 | 3.90 | 26.5 | 48.0 |
| C++ | 15 | **1.0** | 1.50 | 2.90 | 8.50 | 26.0 | 52.3 |

![Deploy frequency by language](../output/charts/deploy-frequency-by-language.svg)

**Early read:** Deployment medians cluster around 1/week regardless of language, but the tail widens for TypeScript/Python cohorts. TypeScript teams pair that with notably faster PR cycle medians (~6 h) than Go (~23 h). Bootstrap 95% CIs for PR medians – TypeScript (4.2 h, 12.5 h) vs Go (17.5 h, 44.1 h) – show non-overlapping ranges, supporting a material gap.

## Scale Effects
Size tiers are based on merged PR counts over the window.

| Tier | Repos | Median DF | P75 DF | P85 DF | P95 DF | Median PR (h) | P75 PR (h) |
| --- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Small (<100 PRs) | 182 | 1.0 | 2.00 | 3.92 | 7.0 | 19.6 | 48.1 |
| Medium (100–499 PRs) | 77 | 1.0 | 2.00 | 2.60 | 5.60 | 18.9 | 30.6 |
| Large (≥500 PRs) | 21 | 1.0 | 5.00 | **19.0** | **225** | **10.1** | 16.5 |

![PR cycle time by size tier](../output/charts/pr-cycle-by-size-tier.svg)

**Outlier note:** The P95 spike for large cohorts is driven by ultra-high-frequency deployments at `getsentry/sentry` (≈320 prod deployments/week) and `PostHog/posthog` (≈225/week). Dropping those two outliers pulls the large-tier P95 down to ≈27.6 deployments/week; we’ll present both raw and trimmed tails so readers understand the automation extremes.

**Takeaway:** High-throughput teams sustain markedly shorter PR cycles (~10 h median) while the deployment tail widens dramatically. This validates the simulator’s assumption that automation enables faster merges, but we need to account for the heavy tail when presenting community baselines.

## MTTR / CFR Prototype (n = 22 repos, exploratory only)
Revert-driven heuristics over a 120-day window (see `output/metrics-mttr-cfr-extended.json`). Fifteen repos recorded at least one revert; the raw median MTTR across those revert-positive repos is **33.0 h** and the median change-failure rate is **26.1 %**. Filtering out the repos listed in `config/repos.anomalies.json` (plus anything auto-flagged for high CFR or sub-weekly deploy volume) pulls the CFR median down to **1.4 %**. Budibase still surfaces the only incident-labelled issue after expanding the label synonym list.

| Repo | Deploys (120d est) | Reverts | Median MTTR (h) | CFR | Flags |
| --- | ---: | ---: | ---: | ---: | --- |
| getsentry/sentry | 5,389.8 | 108 | **2.7** | 2.0% | - |
| ClickHouse/ClickHouse | 33.3 | 70 | 25.3 | 100.0% | high_cfr_outlier, cfr_exceeds_one |
| metabase/metabase | 39.2 | 30 | 152.1 | 76.5% | high_cfr_outlier |
| Budibase/budibase | 75.8 | 17 | 26.8 | 22.4% | - |
| envoyproxy/envoy | 17.3 | 12 | 24.9 | 69.2% | high_cfr_outlier |
| langfuse/langfuse | n/a | 10 | 33.1 | n/a | missing_deploy_window |
| hashicorp/vault | 5.8 | 8 | 117.7 | 100.0% | subweekly_deploy_rate, high_cfr_outlier, cfr_exceeds_one |
| vercel/next.js | 7.5 | 5 | 24.8 | 66.2% | subweekly_deploy_rate, deploy_coverage_sparse, high_cfr_outlier |
| 18F/identity-idp | 36.0 | 4 | 130.1 | 11.1% | - |
| hashicorp/terraform | 6.5 | 3 | 67.8 | 46.3% | subweekly_deploy_rate |
| openstatusHQ/openstatus | 874.8 | 2 | 0.0 | 0.2% | - |
| growthbook/growthbook | 1,626.1 | 2 | 33.0 | 0.1% | - |
| temporalio/temporal | 3.4 | 1 | 238.1 | 29.8% | subweekly_deploy_rate, deploy_coverage_sparse |
| TobikoData/sqlmesh | 71.3 | 1 | 0.0 | 1.4% | - |
| Kong/kong | 1,620.2 | 1 | 5,273.3 | 0.1% | - |

Flagged repos are also tracked in `config/repos.anomalies.json` so downstream scripts can exclude or annotate them without manual grep.

Deploy counts are rescaled to a 120-day equivalent so CFR compares like-for-like with the MTTR window. Raw ratios can exceed 100% when a repo ships more reverts than scaled deploys (see `changeFailureRateRaw` in the JSON); we cap the displayed CFR at 100% and flag the anomaly for follow-up. Median CFR across all revert-positive repos sits at **26.1 %**, but drops to **1.4 %** once we exclude the repos listed in `config/repos.anomalies.json` plus anything flagged for high CFR or sub-weekly deploy volume—evidence that a handful of outliers skew the aggregate.

Repos such as Unleash, PostHog, chakra-ui, and AFFiNE had zero flagged reverts in the window; we keep them in the dataset but treat their MTTR/CFR as “not observed.”

### Literature comparison

| Metric | Wilkes / Rüegger baseline | Wilkes / Rüegger post | Cohort median | Gap vs post | Notes |
| --- | --- | --- | --- | --- | --- |
| Cycle time | 3.2 d | 1.8 d | 0.77 d (18.5 h) | **–1.0 d** faster | Whole-cohort PR cycle-time median from `metrics-summary.json` |
| MTTR | 6.5 h | 2.2 h | 29.9 h (trimmed) | **+27.7 h** slower | Trimmed median excludes anomaly-flagged repos |
| Deployment frequency | 0.7 /wk | 2.1 /wk | 1.0 /wk | **–1.1 /wk** gap | Median deployments/week across 280 repos |
| Change failure rate | 12 % | 5 % | 1.4 % (trimmed) | **–3.6 pp** lower | Trimmed median across revert-positive repos |

**Caveats:**
- MTTR window currently uses 120 days (recent snapshot) while deployment/PR metrics cover 365 days. Align the windows or present separate “recent” charts before publication.
- Revert detection relies on `This reverts commit…` messages; silent rollbacks/feature flags escape the heuristic. For openstatusHQ the two reverts landed within minutes (0.8 min and 3.4 min), explaining the near-zero median.
- ClickHouse’s `cfr_exceeds_one` flag stems from 70 revert commits in the lookback window (57 direct commit reverts, 13 merge rollbacks). That pattern likely mixes production hotfix reversions with backport cleanups; either annotate explicitly or omit from the headline CFR rollup so one team does not dominate the median.
- Repos flagged `subweekly_deploy_rate` (e.g., vercel/next.js) run fewer than one deploy per week in the 120-day slice, so their CFR is sensitive to small revert counts. Consider filtering these from aggregate summaries.
- Incidents are still sparse (1 hit). Need manual label audit and additional synonyms before citing CFR/MTTR in the blog post.
- Coverage nudged up but remains thin (22/280 repos ≈ 7.9%). Expand to ≥30 repos across ecosystems, and consider 365-day MTTR once rate-limit handling is improved.

## Open Questions / TODO
- [x] Cross-check simulator priors (Wilkes, Rüegger) against the aggregate numbers above—load their baseline medians so the deltas are explicit. **Owner:** Carlo (target 2025-10-12).
- [x] Expand MTTR sample to backend-heavy repos (e.g., `Kong/kong`, `temporalio/temporal`) and explore additional revert patterns (`git revert -m` merges).
- [x] Add notebooks/plots for PR size vs cycle time; current snapshots don’t include PR diff statistics.
- [x] Investigate anomalously slow-cycle repositories (`iina/iina`, `wallabag/wallabag`) once their PR activity ramps; flag if they remain outliers. **Resolution:** Both repos remain extreme outliers with medians >2600 h (111+ days) due to critically low PR throughput (n=3 each in 365d window). iina/iina: all 3 PRs took 138–264 days (median 4551 h), single-maintainer bottleneck on `develop` branch. wallabag/wallabag: 2 of 3 PRs took 111–296 days (median 2676 h), third was a 2 h translation bot merge. **Action:** Flag both as `low_pr_throughput` outliers and exclude from cross-ecosystem PR cycle comparisons; they reflect maintainer capacity constraints rather than representative development patterns.
- [ ] Prepare blog-ready charts (language vs cycle time, size tier vs deployment tail) once storytelling angle is locked.
- [x] Decide whether to combine multi-method deployment rows into a single per-repo aggregate or keep them separated but annotate analyses accordingly. **Resolution:** Keep the single-method-per-repo contract. Each repo in `config/repos.sample.json` declares one preferred deploy source (actions/deployments/releases), and the aggregator enforces one row per repo. If coverage is sparse, adjust the config rather than merging sources—avoids double-counting and keeps audit trails clean.

## PR size vs cycle time
The new `output/metrics-pr-size.json` snapshot derives median diff sizes per repo (additions + deletions) alongside the existing PR cycle medians.

- ![PR cycle time vs diff size](../output/charts/pr-cycle-vs-pr-size.svg)

- Simple correlation is weak (Pearson ≈ 0.03 across 265 repos), but bucketed medians still show monotonic drift: repos with median PR ≤24 lines ship in ~10 h, while those above 85 lines land at ~25 h.
- Languages with the largest median diffs skew toward mobile/UI stacks (Swift ≈274 lines median) and infrastructure-heavy Lua (Kong). Go repos cluster around mid-range sizes (≈66 lines) yet still spread widely in cycle time.
- Takeaway: PR sizing alone doesn’t explain the 10× cycle spread—automation and review bandwidth matter more—but the extreme buckets (>85 lines) do pay a 2× cycle-time penalty relative to tiny diffs.

**Caveats:**
- Additions/deletions come from GitHub’s merge metadata and ignore generated files/third-party syncs; we don’t yet mask vendored directories.
- The snapshot uses per-repo medians; within-repo variance is substantial (P90 changes often >400 lines), suggesting future work should sample at PR-granularity.

## Reproducing visuals
- `npm run charts` regenerates the SVG assets under `output/charts/` using `scripts/visualize/generate-charts.ts` so the blog can reference the latest numbers.
